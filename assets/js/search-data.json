{
  
    
        "post0": {
            "title": "Deploy Machine Learning Web Apps for Free",
            "content": "Do you want to deploy your Python code to web for free? Read this post to know the step-by-step procedure to host your web app for free on Heroku cloud. . Deploying a Machine Learning model is a difficult task due to the requirement of large memory and powerful computation. This tutorial focuses on a simple deployment technique that can be used to deploy any Python web app for free. . Read my previous article to learn how to build an ‚ÄúImage classification web app with FastAPI and Tensorflow‚Äù . If you prefer a video tutorial then click on the video below: . Step 1 . First of all, you will need a Heroku id, so go now and register for a free account. . For deploying any Python app on Heroku, we need three files- requirements.txt, runtime.txt, and Procfile. . requirements.txt is a normal text file that contains Python packages required to run the app. . runtime.txt is a text file that will contain the Python Version you want your app to run on. . Procfile will contain the command to launch your web app. For example, you can use . method 1 `python application/server/main.py` or uvicorn if you are deploying a uvicorn server `uvicorn application.server.main:app` . Step 2 . Go to your Heroku dashboard then click on New and create a new app . . Enter your App name and select the Server region that is nearest to your location and click on Create app . . . Step 3 . After you create the app, you will see the deployment methods ‚Äî Heroku Git, GitHub, and Container Registry. I will use the GitHub method. For this just push your code repository to your GitHub account and then connect to GitHub on Heroku. . . Then search the repository and connect it to your Heroku app. . . After this, you will see a deploy button, select the branch of your Git repository that you want to be deployed, and click on deploy. . Then Heroku will automatically start your deployment üéâüéâ After deployment, you can access your app from any web browser üî• . . . Hope this article helped you in the deployment of your web app. If you have some feedback or suggestion please let me know in the comment section. . Follow me on Twitter: https://twitter.com/aniketmaurya . Subscribe to my YouTube channel: https://www.youtube.com/channel/UCRuFsj94hWecPkuEr4f5Xww .",
            "url": "http://blog.aniketmaurya.ml/python/2020/10/15/deploy-python-heroku.html",
            "relUrl": "/python/2020/10/15/deploy-python-heroku.html",
            "date": " ‚Ä¢ Oct 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Building Machine Learning API with FastAPI and Tensorflow",
            "content": "Youtube tutorial version of this blog is also available . FastAPI is a high-performance asynchronous framework for building APIs in Python. It provides support for Swagger UI out of the box. . Source code for this blog is available aniketmaurya/tensorflow-fastapi-starter-pack . hello-world of FastAPI . First, we import FastAPI class and create an object app. This class has useful parameters like we can pass the title and description for Swagger UI. . from fastapi import FastAPI app = FastAPI(title=&#39;Hello world&#39;, description=&#39;This is a hello world example&#39;, version=&#39;0.0.1&#39;) . We define a function and decorate it with @app.get. This means that our API /index supports the GET method. The function defined here is async, FastAPI automatically takes care of async and without async methods by creating a thread pool for the normal def functions and it uses an async event loop for async functions. . @app.get(&#39;/index&#39;) async def hello_world(): return &quot;hello world&quot; . Pydantic support . One of my favorite features offered by FastAPI is Pydantic support. We can define Pydantic models and request-response will be handled by FastAPI for these models. Let‚Äôs create a COVID-19 symptom checker API to understand this. . Covid-19 symptom checker API . We create a request body, it is the format in which the client should send the request. It will be used by Swagger UI. . from pydantic import BaseModel class Symptom(BaseModel): fever: bool = False dry_cough: bool = False tiredness: bool = False breathing_problem: bool = False . Let‚Äôs create a function to assign a risk level based on the inputs. . This is just for learning and should not be used in real life, better consult a doctor. . def get_risk_level(symptom: Symptom): if not (symptom.fever or symptom.dry_cough or symptom.tiredness or symptom.breathing_problem): return &#39;Low risk level. THIS IS A DEMO APP&#39; if not (symptom.breathing_problem or symptom.dry_cough): if symptom.fever: return &#39;moderate risk level. THIS IS A DEMO APP&#39; if symptom.breathing_problem: return &#39;High-risk level. THIS IS A DEMO APP&#39; return &#39;THIS IS A DEMO APP&#39; . Let‚Äôs create the API for checking the symptoms . @app.post(&#39;/api/covid-symptom-check&#39;) def check_risk(symptom: Symptom): return get_risk_level(symptom) . Image recognition API . We will create an API to classify images, we name it predict/image. We will use Tensorflow for creating the image classification model. . Tutorial for Image Classification with Tensorflow . We create a function load_model, which will return a MobileNet CNN Model with pre-trained weights i.e. it is already trained to classify 1000 unique categories of images. . import tensorflow as tf def load_model(): model = tf.keras.applications.MobileNetV2(weights=&quot;imagenet&quot;) print(&quot;Model loaded&quot;) return model model = load_model() . We define a predict function that will accept an image and returns the predictions. We resize the image to 224x224 and normalize the pixel values to be in [-1, 1]. . from tensorflow.keras.applications.imagenet_utils import decode_predictions . decode_predictions is used to decode the class name of the predicted object. Here we will return the top-2 probable class. . def predict(image: Image.Image): image = np.asarray(image.resize((224, 224)))[..., :3] image = np.expand_dims(image, 0) image = image / 127.5 - 1.0 result = decode_predictions(model.predict(image), 2)[0] response = [] for i, res in enumerate(result): resp = {} resp[&quot;class&quot;] = res[1] resp[&quot;confidence&quot;] = f&quot;{res[2]*100:0.2f} %&quot; response.append(resp) return response . Now we will create an API /predict/image which supports file upload. We will filter the file extension to support only jpg, jpeg, and png format of images. . We will use Pillow to load the uploaded image. . def read_imagefile(file) -&gt; Image.Image: image = Image.open(BytesIO(file)) return image . @app.post(&quot;/predict/image&quot;) async def predict_api(file: UploadFile = File(...)): extension = file.filename.split(&quot;.&quot;)[-1] in (&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;) if not extension: return &quot;Image must be jpg or png format!&quot; image = read_imagefile(await file.read()) prediction = predict(image) return prediction . Final code . import uvicorn from fastapi import FastAPI, File, UploadFile from application.components import predict, read_imagefile app = FastAPI() @app.post(&quot;/predict/image&quot;) async def predict_api(file: UploadFile = File(...)): extension = file.filename.split(&quot;.&quot;)[-1] in (&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;) if not extension: return &quot;Image must be jpg or png format!&quot; image = read_imagefile(await file.read()) prediction = predict(image) return prediction @app.post(&quot;/api/covid-symptom-check&quot;) def check_risk(symptom: Symptom): return symptom_check.get_risk_level(symptom) if __name__ == &quot;__main__&quot;: uvicorn.run(app, debug=True) . FastAPI documentation is the best place to learn more about core concepts of the framework. . . . Hope you liked the article. . Feel free to ask your questions in the comments or reach me out personally . üëâ Twitter: https://twitter.com/aniketmaurya . üëâ Linkedin: https://linkedin.com/in/aniketmaurya .",
            "url": "http://blog.aniketmaurya.ml/python/2020/07/26/fastapi-tf-webapp.html",
            "relUrl": "/python/2020/07/26/fastapi-tf-webapp.html",
            "date": " ‚Ä¢ Jul 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Probability vs Likelihood vs Maximum Likelihood",
            "content": ". Probability . Probability is the quantitative measure of certainty of an event. Mathematically, probability $P$ for an event $E$ is defined as: . $P(E) = frac{number of times event E occurred}{Total number of events}$ . Consider a fair die, possible outcomes of rolling the die can be 1 to 6. Where probability of each outcome is 1/6. . $P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = 1/6$ What if we roll two dice together? What will be the probability that both dice will get the same digits, say 6? . $P(dice_1=6 cap dice_2=6) = P(dice_1=6) times P(dice_2=6)= frac{1}{6} times frac{1}{6}$ . What if one of the dice is biased towards 6, i.e. $P(dice_1=6) = 1/4 and P(dice_2=6)=1/6$ . The probability will become $P(dice_1=6 cap dice_2=6) = 1/6 times 1/4$ . Joint Probability . Joint probability is the probability of two events occurring simultaneously. Rolling two dice together is an example of Joint Probability. . If the two events E1 and E2 are independent of each other then the joint probability is multiplication of P(E1) and P(E2). . $P(E_1 and E_2) = P(E_1) times P(E_2)$ . P(A given B) is denoted as P(A|B) . If the two events E1 and E2 are not independent then the joint probability is: . $P(E_1 and E_2) = P(E_1|E_2)P(E_2) = P(E_2|E_1)P(E_1)$ Joint probability is symmetric . P(A,B) = P(B, A) P(A,B) = P(B|A)P(A) P(B,A) = P(A|B)P(B) P(A|B)P(B) = P(B|A)P(A) . $P(A|B) = frac{P(B|A)P(A)}{P(B)}$ . Conditional Probability . It is the probability of one event when occurence of the other is given. . Consider two events A and B, given that B has occurred. Then the probability of A given B is: . $P(A|B) = frac{P(B|A)P(B)}{P(A)} = frac{P(A, B)}{P(A)}$ . Maximum Likelihood . Maximum Likelihood is used to find the normal distribution of the given data. We estimate $ mu$ and $ sigma$ for the distribution. . References . https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/ . | https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1 . | .",
            "url": "http://blog.aniketmaurya.ml/statistics/machine%20learning/2020/04/29/Probability-vs-Likelihood.html",
            "relUrl": "/statistics/machine%20learning/2020/04/29/Probability-vs-Likelihood.html",
            "date": " ‚Ä¢ Apr 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Normalization",
            "content": "Normalization in Deep Learning . Normalization is an important technique widely used in Deep Learning to achieve better results in less time. . Why do we need to Normalize in the first place? . Covariate Shift: Most of the time the training dataset is very different from the real dataset. Suppose, a CNN model is trained to classify cats. But the training dataset only had images of black cats. So, if the model is fed with an image of a white cat it may not predict correctly. This phenomenon is called Covariate-Shift. . In the graph below, the training data mostly falls on a linear function. But after including test data it looks more like a quadratic distribution. . Source: http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf . Generally in image datasets, the distribution can change because of change in camera resolutions or any other environmental change. .",
            "url": "http://blog.aniketmaurya.ml/deep%20learning/machine%20learning/2020/04/26/Normalization.html",
            "relUrl": "/deep%20learning/machine%20learning/2020/04/26/Normalization.html",
            "date": " ‚Ä¢ Apr 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "tf.data: Creating data input pipelines",
            "content": "Are you not able to load your NumPy data into memory? Does your model have to wait for data to be loaded after each epoch? Is your Keras DataGenerator slow? . TensorFlow tf.data API allows building complex input pipelines. It easily handles a large amount of data and can read different formats of data while allowing complex data transformations. . Why do we need tf.data? . A training step involves the following steps: . File reading | Fetch or parse data | Data transformation | Using the data to train the model. | source: Tensorflow . If you have a large amount of data and you‚Äôre unable to load it into the memory, you may want to use Generators. But Generators has limited portability and scalability. . After every epoch, you will wait for data to be transformed into a consumable format by the model and during that time your model sits idle, not doing any training. This leads to low CPU and GPU utilization. . One solution to handle this is to prefetch your data in advance and you won‚Äôt have to wait for data to be loaded. . source: Tensorflow . tf.data is a data input pipeline building API than you can use to easily build your data pipeline. Whether you want to read data from local files or even if your data is stored remotely. . Loading data for classification . To train an image classification model, we create a CNN model and feed our data to the model. I want to train a Cats vs Dogs classifier and my data is stored in the following folder structure. . data ‚îî‚îÄ‚îÄ train ‚îú‚îÄ‚îÄ cat -&gt; contains images of cats ‚îî‚îÄ‚îÄ dog -&gt; contains images of dogs* . We first find the path of all the images- . from glob import glob import tensorflow as tf image_path_list = glob(&#39;data/train/*/*.jpg&#39;) data = tf.data.Dataset.list_files(image_path_list) . tf.data.Dataset.list_files converts the list returned by glob method to the Dataset object. Now, we will load the images and their class. . def load_images(path): image = tf.io.read_file(path) image = tf.io.decode_image(image) label = tf.strings.split(path, os.path.sep)[-2] return image, label data = data.map(load_images) . So, the data object now has images and labels. But this is not it, we will have to resize the image, preprocess and apply transformations. . def preprocess(image, label): image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH)) image = tf.image.random_flip_left_right(image) image = tf.image.random_flip_up_down(image) image /= 255. image -= 0.5 return image, label data = data.map(preprocess) . I have created a small library named Chitra, based on tf.data that can be used to skip all these steps. . from chitra import dataloader as dl path = &#39;./data/train&#39; train_dl = dl.Clf() data = train_dl.from_folder(path, target_shape=(224, 244), shuffle = True) # to visualize the data train_dl.show_batch(6, figsize=(6,6)) . You can just specify the path of your data and it will be loaded with the target size. . . You can find my code at https://github.com/aniketmaurya/chitra .",
            "url": "http://blog.aniketmaurya.ml/tensorflow/2020/04/08/tf.data-Creating-Data-Input-Pipelines.html",
            "relUrl": "/tensorflow/2020/04/08/tf.data-Creating-Data-Input-Pipelines.html",
            "date": " ‚Ä¢ Apr 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Regression from Scratch",
            "content": "Import libraries . import numpy as np import random import matplotlib.pyplot as plt . Data . X = [*range(1, 51)] Y = list(map(lambda x: 2*x + 5, X)) . Univariate Regression . $h( theta) = theta*X + b$ . MSE cost function . $ sum (h(x) - y)^2$ . Gradient Descent . repeat { √ò = √ò - ‚àÜJ(√ò) = √ò - LR*1/m * sum((h(√ò, b) - Y)*X) b = b - ‚àÜJ(b) = b - LR*1/m * sum((h(√ò, b) - Y)) } . def mse(y_true, y_pred): cost = 0 m = len(y_pred) for i in range(m): cost += (y_pred[i] - y_true[i]) ** 2 return cost/(2*m) def der_mse(y_true, y_pred): der_cost = 0 m = len(y_pred) for i in range(m): der_cost += (y_pred[i] - y_true[i]) return der_cost def predict(x): return w*x + b . # Intialization of variables m = len(X) LR = 0.01 w,b =0,0.1 epochs = 10000 # Training total_cost = [] for i in range(epochs): y_pred = [] epoch_cost = [] for num, data in enumerate(zip(X, Y)): x, y = data y_pred = [] y_pred.append(w*x + b) cost = mse(Y[num:num+1], y_pred) epoch_cost.append(cost) der_cost = der_mse(Y[num:num+1], y_pred) w -= LR * (1/m) * der_cost * x b -= LR * (1/m) * der_cost total_cost.append(np.mean(epoch_cost)) if i%500==0: print(f&#39;epoch:{i} t tcost:{cost}&#39;) . epoch:0 cost:0.024546020195931887 epoch:500 cost:0.0035238913511105277 epoch:1000 cost:0.0004771777468473895 epoch:1500 cost:6.461567040474519e-05 epoch:2000 cost:8.749747634800157e-06 epoch:2500 cost:1.1848222450189964e-06 epoch:3000 cost:1.604393419109384e-07 epoch:3500 cost:2.1725438173628743e-08 epoch:4000 cost:2.9418885555175706e-09 epoch:4500 cost:3.983674896607656e-10 epoch:5000 cost:5.3943803161575866e-11 epoch:5500 cost:7.30464704919418e-12 epoch:6000 cost:9.891380608202818e-13 epoch:6500 cost:1.3394131683086816e-13 epoch:7000 cost:1.8137281109430194e-14 epoch:7500 cost:2.4560089530711338e-15 epoch:8000 cost:3.3257381016463754e-16 epoch:8500 cost:4.5034718706313674e-17 epoch:9000 cost:6.09814092196085e-18 epoch:9500 cost:8.25761584212193e-19 . predict(2), predict(9) . (8.999999990490096, 22.999999991911498) . w, b . (2.000000000203057, 4.999999990083981) . plt.plot(total_cost) plt.show() .",
            "url": "http://blog.aniketmaurya.ml/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "relUrl": "/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "date": " ‚Ä¢ Mar 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) &lt;ipython-input-6-ca0c30357ca1&gt; in &lt;module&gt; 1 # single-value selection over [Major_Genre, MPAA_Rating] pairs 2 # use specific hard-wired values as the initial selected values -&gt; 3 selection = alt.selection_single( 4 name=&#39;Select&#39;, 5 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], NameError: name &#39;alt&#39; is not defined . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . NameError Traceback (most recent call last) &lt;ipython-input-7-b50f012c6ec6&gt; in &lt;module&gt; -&gt; 1 alt.Chart(movies).mark_circle().add_selection( 2 alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) 3 ).encode( 4 x=&#39;Rotten_Tomatoes_Rating:Q&#39;, 5 y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement NameError: name &#39;alt&#39; is not defined . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . NameError Traceback (most recent call last) &lt;ipython-input-8-84286f3c16cf&gt; in &lt;module&gt; 1 # select a point for which to provide details-on-demand -&gt; 2 label = alt.selection_single( 3 encodings=[&#39;x&#39;], # limit selection to x-axis value 4 on=&#39;mouseover&#39;, # select on mouseover events 5 nearest=True, # select data point nearest the cursor NameError: name &#39;alt&#39; is not defined . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "http://blog.aniketmaurya.ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
            "content": "EfficientDet, a highly efficient and scalable state of the art object detection model developed by Google Research, Brain Team. It is not just a single model. It has a family of detectors which achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors. . EfficientDet paper has mentioned its 7 family members. . Comparison of EfficientDet detectors[0‚Äì6] with other SOTA object detection models. . Source: arXiv:1911.09070v1 . Quick Overview of the Paper . EfficientNet is the backbone architecture used in the model. EfficientNet is also written by the same authors at Google. Conventional CNN models arbitrarily scaled network dimensions- width, depth and resolution. EfficientNet uniformly scales each dimension with a fixed set of scaling coefficients. It surpassed SOTA accuracy with 10x efficiency. . | BiFPN: While fusing (applying residual or skip connections) different input features, most of the works simply summed them up without any distinction. Since both input features are at the different resolutions they don‚Äôt equally contribute to the fused output layer. The paper proposes a weighted bi-directional feature pyramid network (BiFPN), which introduces learnable weights to learn the importance of different input features. . | Compound Scaling: For higher accuracy previous object detection models relied on ‚Äî bigger backbone or larger input image sizes. Compound Scaling is a method that uses a simple compound coefficient œÜ to jointly scale-up all dimensions of the backbone network, BiFPN network, class/box network, and resolution. . | Combining EfficientNet backbones with our propose BiFPN and compound scaling, we have developed a new family of object detectors, named EfficientDet, which consistently achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors. . BiFPN . Source: arXiv:1911.09070v1 ‚Äî figure 2 . Conventional FPN (Feature Pyramid Network) is limited by the one-way information flow. PANet added an extra bottom-up path for information flow. PANet achieved better accuracy but with the cost and more parameters and computations. The paper proposed several optimizations for cross-scale connections: . Remove Nodes that only have one input edge. If a node has only one input edge with no feature fusion, then it will have less contribution to the feature network that aims at fusing different features. . | Add an extra edge from the original input to output node if they are at the same level, in order to fuse more features without adding much cost. . | Treat each bidirectional (top-down &amp; bottom-up) path as one feature network layer, and repeat the same layer multiple times to enable more high-level feature fusion. . | Weighted Feature Fusion . While multi-scale fusion, input features are not simply summed up. The authors proposed to add additional weight for each input during feature fusion and let the network to learn the importance of each input feature. Out of three weighted fusion approaches ‚Äî Unbounded fusion: . Source: https://arxiv.org/abs/1911.09070 . Where W is a learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel). Since the scalar weight is unbounded, it could potentially cause training instability. So, Softmax-based fusion was tried for normalized weights. Softmax-based fusion: . . As softmax normalizes the weights to be the probability of range 0 to 1 which can denote the importance of each input. The softmax leads to a slowdown on GPU. Fast normalized fusion: . . –Ñ is added for numeric stability. It is 30% faster on GPU and gave almost as accurate results as softmax. . Final BiFPN integrates both the bidirectional cross-scale connections and the fast normalized fusion. . EfficientDet Architecture . Source: arXiv:1911.09070v1 ‚Äî figure 3 . EfficientDet follows one-stage-detection paradigm. A pre-trained EfficientNet backbone is used with BiFPN as the feature extractor. BiFPNN takes {P3, P4, P5, P6, P7} features from the EfficientNet backbone network and repeatedly applies bidirectional feature fusion. The fused features are fed to a class and bounding box network for predicting object class and bounding box. . References . EfficientDet: Scalable and Efficient Object Detection . EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks . Path Aggregation Network for Instance Segmentation . Deep Residual Learning for Image Recognition . . . Hope you liked the article. . üëâ Twitter: https://twitter.com/aniketmaurya üëâ Mail: aniketmaurya@outlook.com .",
            "url": "http://blog.aniketmaurya.ml/object%20detection/2020/01/13/EfficientDet.html",
            "relUrl": "/object%20detection/2020/01/13/EfficientDet.html",
            "date": " ‚Ä¢ Jan 13, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Image Classification with Tensorflow 2.x",
            "content": "Image Classification with TF 2 . Unlike previous versions, TensorFlow 2.0 is coming out with some major changes. It is going to be more pythonic and no need to turn on eager execution explicitly. With tight integration of Keras now it will focus on simplicity and ease of use. . Keras is a high-level API that allows to easily build, train, evaluate and execute all sorts of neural networks. Keras was developed by Fran√ßois Chollet and open-sourced in March 2015. With its simplicity and easy-to-use feature, it gained popularity very quickly. Tensorflow comes with its own implementation of Keras with some TF specific features. . Keras can run on top of MXNet, CNTK or Theano. . . Building a simple image classifier . We will create a simple Neural Networks architecture for image classification. Fashion MNIST is a collection of 70,000 grayscale images of 28x28 pixel each, with 10 classes of different clothing items. We will train our Neural Network on this dataset. . CNN performs better than Dense NN for image classification both in terms of time and accuracy. I have used Dense NN architecture here for demonstration. . Check this article to learn about Convolutional Neural Networks. . Import libraries and download F-MNIST dataset. . import tensorflow as tf from tensorflow import keras *# tensorflow implementation of keras* import matplotlib.pyplot as plt . Download dataset with Keras utility function* . fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() print(X_train_full.shape) . (60000, 28, 28) . It is always a good practice to split the dataset into training, validation and test set. Since we already have our test set so let‚Äôs create a validation set. We can scale the pixel intensities of the data to the 0‚Äì1 range by dividing 255.0. Scaling leads to better gradient update. . X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0 y_valid, y_train = y_train_full[:5000], y_train_full[5000:] . We can view any photo using matplotlib. . plt.imshow(X_train[5]) . . Create a model using Keras Sequential API . Now it‚Äôs the time to build our simple image classification Artificial Neural Networks. . model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . If you didn‚Äôt get it then don‚Äôt worry, let me explain the code line by line. . The Sequential model is a linear stack of layers, connected sequentially. . The next layer, i.e. **Flatten **is just converting the 28x28 dimension array into a 1D array. If it receives input data X, then it computes X.reshape(-1, 1). It takes an **input_shape **argument to specify the size of the input data. However, input_shape can be automatically detected by Keras. . The** Dense **layer is the fully-connected neurons in the neural networks. Here, there are two hidden layers with 300 neurons in first and 100 neurons in the second hidden layer respectively. . The last Dense layer made up of 10 neurons in the output layer. It is responsible for calculating loss and predictions. . Compiling the model . Keras has a compile() method which specifies loss function to use, optimizer, and metrics. . model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=&quot;sgd&quot;, metrics=[&quot;accuracy&quot;]) . Train and Evaluate . After the model compilation, we can all fit() method by specifying the epochs, batch size, etc. . Training model . history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid*))* . This method will train the model for 30 epochs. Train loss, Validation loss and train accuracy, validation accuracy can be found in history.history. . Loss visualization . We can create a visualization for the learning curve using history. . import pandas as pd pd.DataFrame(history.history).plot(figsize=(8, 5)) plt.grid(True) plt.gca().set_ylim(0, 1) *# set the vertical range to [0-1]* plt.show() . Source: ‚ÄúHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow‚Äù . We can see that the space between validation and training curves are small that‚Äôs why there isn‚Äôt overfitting problem. . Now, we can try out different hyperparameters to achieve more accuracy on the dataset. . Model Evaluation . If you are satisfied with the training and validation accuracy then evaluate it on the test set. . model.evaluate(X_test, Y_test) . Accuracy on test set might be lower than on validation set because the hyperparameters are tuned for validation set. . Save the trained Model . After you have trained and evaluated your NN model on test set you can download your model using Keras tf.keras.models.save_model method and then can load it anytime for inference. . tf.keras.models.save_model(&quot;my_image_classifier&quot;) . It saves both the model‚Äôs architecture and the value of all the model parameters for every layer (All trained weights and biases). This saved_model can be loaded to TF serving for deployement purpose. . If you want to use your trained model for inference, just load it: . model = tf.keras.models.load_model(&quot;my_image_classifier&quot;) . Now, it‚Äôs time to train different datasets on your own. Good Luck üòÑ! . . . Recommended Resources . Deep learning specialization (Coursera) . | ‚ÄúHands-On Machine Learning with Scikit-Learn and TensorFlow‚Äù by Aur√©lien G√©ron (Book from O‚ÄôReilly) . | You can contact me at twitter.com/aniketmaurya or drop an üìß at aniketmaurya@outlook.com .",
            "url": "http://blog.aniketmaurya.ml/tensorflow/deep%20learning/2019/05/12/image-classification-with-tf2.html",
            "relUrl": "/tensorflow/deep%20learning/2019/05/12/image-classification-with-tf2.html",
            "date": " ‚Ä¢ May 12, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Face Recognition",
            "content": "AI is revolutionizing the world. Face recognition is one such spectrum of it. Almost everyone uses face recognition systems ‚Äî on our mobile, Facebook, Photo gallery apps or advanced security cameras. Learn how these systems are able to recognize our faces. . This article is inspired by the deeplearning.ai course on FaceNet. . . Face Verification vs. Face Recognition . Face Verification checks ‚Äúis this the claimed person?‚Äù. For example, in school, you go with your ID card and the invigilator verifies your face with the ID card. This is Face Verification. A mobile phone that unlocks using our face is also using face verification. It is 1:1 matching problem. . Now suppose the invigilator knows everyone by their name. So, you decide to go there without an ID card. The invigilator identifies your face and lets you in. This is Face Recognition. Face Recognition deals with ‚Äúwho is this person?‚Äù problem. We can say that it is a 1:K problem. . Why pixel-by-pixel comparison of images is¬†a¬†bad¬†idea? . A simple way for face verification can be comparing two images pixel-by-pixel and if the threshold between images is less than a threshold then we can say that they‚Äôre the same person. But since the pixel values in an image change dramatically even with a slight change of light, position or orientation. So, this method doesn‚Äôt work well. . Face Embedding come to the rescue . The embedding is represented by f (x), a d-dimensional vector. It encodes an image ‚Äòx‚Äô into a d-dimensional Euclidean space. The face embedding of two images of the same person are similar to each other and that of different persons are very different. . In ConvNet architectures, the initial layers learn to recognize basic patterns like straight lines, edges, circles, etc. and the deeper layers learn to recognize more complex patterns like numbers, faces, cars, etc. . To get our embedding we feed the image into a pre-trained model then run a forward propagation and extract the vector from some deeper Fully-Connected layer. You can learn the basics of CNN here. . How to finetune embeddings? . The Triplet Loss function takes the responsibility to push the embeddings of two images of the same person (Anchor and Positive) closer together while pulling the embeddings of two images of different persons (Anchor, Negative) further apart. . Source: Coursera . $||f(X1) ‚Äî f(X2)||¬≤$ is the degree of similarity between image X1 and X2. If it‚Äôs smaller than a chosen threshold then both are the same person. . If you‚Äôre wondering ‚Äúwhat this $|A|$ weird symbol is?‚Äù, it‚Äôs called Frobenius Norm. . The distance between Anchor and Positive images should be less and the distance between Anchor and Negative images should be high. . i.e. $‚à•f(Anchor) ‚Äî f(Positive)‚à•¬≤ ‚â§ ‚à•f(Anchor) ‚Äî f(Negative)‚à•¬≤$. . Training a Face recognition model is computationally expensive so it‚Äôs recommended to download a pre-trained model. . Start with creating a database of persons containing an embedding vector for each. . #create a dictionary database db = dict() #encoding(image_path) converts image to embedding db[&#39;person1&#39;] = encoding(&#39;person1.jpg&#39;) db[&#39;person2&#39;] = encoding(&#39;person2.jpg&#39;) db[&#39;person3&#39;] = encoding(&#39;person3.jpg&#39;) db[&#39;person4&#39;] = encoding(&#39;person4.jpg&#39;) db[&#39;person5&#39;] = encoding(&#39;person5.jpg&#39;) . Face Verification . Now that we have created our database, we can define a function that accepts image embedding and name of the person as the argument and it will verify if they are the same person. . def verify(embedding, person_name): # numpy.linalg.norm calculates the Frobenius Norm dist = np.linalg.norm(embedding - db[person_name]) # Chosen threshold is 0.7 if dist &lt; 0.7: print(&quot;Verified! Welcome &quot; + person_name) else: print(&quot;Person name and face didn&#39;t match!&quot;) . Hurray üòé! We created our Face Verification system. Now let‚Äôs create the Face Recognition System. If you remember, a person doesn‚Äôt need any ID in the face recognition system. He just needs to show his face to the camera. . Face Recognition . In Face recognition, the distance will be calculated for all the images in the database against the input embedding and the smallest distance has to be searched. . def recognize_me(input_embedding): # Set min_dist to infinity min_dist = np.inf # Iterate over the database to calulate distance for each person* for (name, emb) in db.items(): # Compute the distance* curr_dist = np.linalg.norm(input_embedding - emb) # identity is set to the name of the person from the database whose distance is smallest against the input encoding if curr_dist &lt; min_dist: min_dist = curr_dist identity = name if min_dist &gt; 0.7: print(&quot;Sorry! You‚Äôre not in the database.&quot;) else: print (&quot;Hi! Welcome &quot; + identity) . Congratulations!! üëèüëè You have created your own Face Recognition system. . You can provide your feedback in comment section below. . Follow me on Twitter .",
            "url": "http://blog.aniketmaurya.ml/tensorflow/face%20recognition/2019/01/07/face-recognition.html",
            "relUrl": "/tensorflow/face%20recognition/2019/01/07/face-recognition.html",
            "date": " ‚Ä¢ Jan 7, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Aniket Maurya is a professional Machine Learning and Computer Vision Engineer. He started his journey in Computer Vision at COVIAM Technologies to build Face Recognition system. . Aniket is experienced in deploying robust and scalable Machine Learning &amp; Deep Learning services. He is actively working on various Computer Vision Deep Learning projects at COVIAM, some of those are - Watermark detection, Explicit Content detection, Face Spoof detection, Image Restoration with GANs. . Aniket likes to blog on various Deep Learning related topics. Contact Aniket for freelance or consultation on building or deploying AI based solutions. . /aniketmaurya . /aniketmaurya . /aniketmaurya . This website is powered by fastpages .",
          "url": "http://blog.aniketmaurya.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "http://blog.aniketmaurya.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}