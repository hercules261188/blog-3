{
  
    
        "post0": {
            "title": "Linear Regression from Scratch",
            "content": "Import libraries . import numpy as np import random import matplotlib.pyplot as plt . Data . X = [*range(1, 51)] Y = list(map(lambda x: 2*x + 5, X)) . Univariate Regression . $h( theta) = theta*X + b$ . MSE cost function . $ sum (h(x) - y)^2$ . Gradient Descent . repeat { √ò = √ò - ‚àÜJ(√ò) = √ò - LR*1/m * sum((h(√ò, b) - Y)*X) b = b - ‚àÜJ(b) = b - LR*1/m * sum((h(√ò, b) - Y)) } . def mse(y_true, y_pred): cost = 0 m = len(y_pred) for i in range(m): cost += (y_pred[i] - y_true[i]) ** 2 return cost/(2*m) def der_mse(y_true, y_pred): der_cost = 0 m = len(y_pred) for i in range(m): der_cost += (y_pred[i] - y_true[i]) return der_cost def predict(x): return w*x + b . # Intialization of variables m = len(X) LR = 0.01 w,b =0,0.1 epochs = 10000 # Training total_cost = [] for i in range(epochs): y_pred = [] epoch_cost = [] for num, data in enumerate(zip(X, Y)): x, y = data y_pred = [] y_pred.append(w*x + b) cost = mse(Y[num:num+1], y_pred) epoch_cost.append(cost) der_cost = der_mse(Y[num:num+1], y_pred) w -= LR * (1/m) * der_cost * x b -= LR * (1/m) * der_cost total_cost.append(np.mean(epoch_cost)) if i%500==0: print(f&#39;epoch:{i} t tcost:{cost}&#39;) . epoch:0 cost:0.024546020195931887 epoch:500 cost:0.0035238913511105277 epoch:1000 cost:0.0004771777468473895 epoch:1500 cost:6.461567040474519e-05 epoch:2000 cost:8.749747634800157e-06 epoch:2500 cost:1.1848222450189964e-06 epoch:3000 cost:1.604393419109384e-07 epoch:3500 cost:2.1725438173628743e-08 epoch:4000 cost:2.9418885555175706e-09 epoch:4500 cost:3.983674896607656e-10 epoch:5000 cost:5.3943803161575866e-11 epoch:5500 cost:7.30464704919418e-12 epoch:6000 cost:9.891380608202818e-13 epoch:6500 cost:1.3394131683086816e-13 epoch:7000 cost:1.8137281109430194e-14 epoch:7500 cost:2.4560089530711338e-15 epoch:8000 cost:3.3257381016463754e-16 epoch:8500 cost:4.5034718706313674e-17 epoch:9000 cost:6.09814092196085e-18 epoch:9500 cost:8.25761584212193e-19 . predict(2), predict(9) . (8.999999990490096, 22.999999991911498) . w, b . (2.000000000203057, 4.999999990083981) . plt.plot(total_cost) plt.show() .",
            "url": "https://aniketmaurya.github.io/blog/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "relUrl": "/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "date": " ‚Ä¢ Mar 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://aniketmaurya.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Image Classification with Tensorflow 2.x",
            "content": "Image Classification with TF 2 . Unlike previous versions, TensorFlow 2.0 is coming out with some major changes. It is going to be more pythonic and no need to turn on eager execution explicitly. With tight integration of Keras now it will focus on simplicity and ease of use. . Keras is a high-level API that allows to easily build, train, evaluate and execute all sorts of neural networks. Keras was developed by Fran√ßois Chollet and open-sourced in March 2015. With its simplicity and easy-to-use feature, it gained popularity very quickly. Tensorflow comes with its own implementation of Keras with some TF specific features. . Keras can run on top of MXNet, CNTK or Theano. . . Building a simple image classifier . We will create a simple Neural Networks architecture for image classification. Fashion MNIST is a collection of 70,000 grayscale images of 28x28 pixel each, with 10 classes of different clothing items. We will train our Neural Network on this dataset. . CNN performs better than Dense NN for image classification both in terms of time and accuracy. I have used Dense NN architecture here for demonstration. . Check this article to learn about Convolutional Neural Networks. . Import libraries and download F-MNIST dataset. . import tensorflow as tf from tensorflow import keras *# tensorflow implementation of keras* import matplotlib.pyplot as plt . Download dataset with Keras utility function* . fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() print(X_train_full.shape) . (60000, 28, 28) . It is always a good practice to split the dataset into training, validation and test set. Since we already have our test set so let‚Äôs create a validation set. We can scale the pixel intensities of the data to the 0‚Äì1 range by dividing 255.0. Scaling leads to better gradient update. . X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0 y_valid, y_train = y_train_full[:5000], y_train_full[5000:] . We can view any photo using matplotlib. . plt.imshow(X_train[5]) . . Create a model using Keras Sequential API . Now it‚Äôs the time to build our simple image classification Artificial Neural Networks. . model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) . If you didn‚Äôt get it then don‚Äôt worry, let me explain the code line by line. . The Sequential model is a linear stack of layers, connected sequentially. . The next layer, i.e. **Flatten **is just converting the 28x28 dimension array into a 1D array. If it receives input data X, then it computes X.reshape(-1, 1). It takes an **input_shape **argument to specify the size of the input data. However, input_shape can be automatically detected by Keras. . The** Dense **layer is the fully-connected neurons in the neural networks. Here, there are two hidden layers with 300 neurons in first and 100 neurons in the second hidden layer respectively. . The last Dense layer made up of 10 neurons in the output layer. It is responsible for calculating loss and predictions. . Compiling the model . Keras has a compile() method which specifies loss function to use, optimizer, and metrics. . model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=&quot;sgd&quot;, metrics=[&quot;accuracy&quot;]) . Train and Evaluate . After the model compilation, we can all fit() method by specifying the epochs, batch size, etc. . Training model . history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid*))* . This method will train the model for 30 epochs. Train loss, Validation loss and train accuracy, validation accuracy can be found in history.history. . Loss visualization . We can create a visualization for the learning curve using history. . import pandas as pd pd.DataFrame(history.history).plot(figsize=(8, 5)) plt.grid(True) plt.gca().set_ylim(0, 1) *# set the vertical range to [0-1]* plt.show() . Source: ‚ÄúHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow‚Äù . We can see that the space between validation and training curves are small that‚Äôs why there isn‚Äôt overfitting problem. . Now, we can try out different hyperparameters to achieve more accuracy on the dataset. . Model Evaluation . If you are satisfied with the training and validation accuracy then evaluate it on the test set. . model.evaluate(X_test, Y_test) . Accuracy on test set might be lower than on validation set because the hyperparameters are tuned for validation set. . Save the trained Model . After you have trained and evaluated your NN model on test set you can download your model using Keras tf.keras.models.save_model method and then can load it anytime for inference. . tf.keras.models.save_model(&quot;my_image_classifier&quot;) . It saves both the model‚Äôs architecture and the value of all the model parameters for every layer (All trained weights and biases). This saved_model can be loaded to TF serving for deployement purpose. . If you want to use your trained model for inference, just load it: . model = tf.keras.models.load_model(&quot;my_image_classifier&quot;) . Now, it‚Äôs time to train different datasets on your own. Good Luck üòÑ! . . . Recommended Resources . Deep learning specialization (Coursera) . | ‚ÄúHands-On Machine Learning with Scikit-Learn and TensorFlow‚Äù by Aur√©lien G√©ron (Book from O‚ÄôReilly) . | You can contact me at twitter.com/aniketmaurya or drop an üìß at aniketmaurya@outlook.com .",
            "url": "https://aniketmaurya.github.io/blog/tensorflow/deep%20learning/2019/05/12/image-classification-with-tf2.html",
            "relUrl": "/tensorflow/deep%20learning/2019/05/12/image-classification-with-tf2.html",
            "date": " ‚Ä¢ May 12, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Aniket Maurya is a professional Machine Learning and Computer Vision Engineer. He started working in Computer Vision as intern at COVIAM Technologies to build Face Recognition system for attendance. He is currently working in various Computer Vision Deep Learning projects at COVIAM. . Aniket writes blog on various Deep Learning topics. You can contact him for freelance work. . This website is powered by fastpages .",
          "url": "https://aniketmaurya.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://aniketmaurya.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}